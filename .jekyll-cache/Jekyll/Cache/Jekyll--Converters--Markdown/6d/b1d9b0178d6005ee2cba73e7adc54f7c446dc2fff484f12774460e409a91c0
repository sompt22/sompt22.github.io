I"&<p>SOMPT22 is a Multi-Object Tracking Dataset.</p>

<p><a href="https://orcid.org/0000-0003-3362-5747" title="Fatih Emre Simsek">Fatih Emre Simsek</a> <a href="https://orcid.org/0000-0003-3246-3176" title="Cevahir Cigla">Cevahir Cigla</a> <a href="https://orcid.org/0000-0003-3362-5747" title="Koray Kayabol">Koray Kayabol</a></p>

<figure class="figure  figure--center">
  <img class="image" src="/assets/paper_pics/tumbnail.png" alt="" width="1920" height="1000" />
  
</figure>

<p><a class="button" href="https://github.com/sompt" style="background: #0366d6">Fork it  <svg width="16" height="16" class="icon  icon--github" role="img" alt="github"><title>github</title><use xlink:href="#github" fill="CurrentColor"></use></svg>
</a>
 <a class="button" href="https://twitter.com/" style="background: #0d94e7">Tweet it  <svg width="16" height="16" class="icon  icon--twitter" role="img" alt="twitter"><title>twitter</title><use xlink:href="#twitter" fill="CurrentColor"></use></svg>
</a>
 <a class="button" href="https://arxiv.org/abs/2208.02580" style="background: #0d94e7">Paper  <svg width="16" height="16" class="icon  icon--link" role="img" alt="link"><title>link</title><use xlink:href="#link" fill="CurrentColor"></use></svg>
</a></p>

<h2 id="abstract">Abstract</h2>

<p>Multi-object tracking (MOT) has been dominated by the use of track by detection approaches due to the success of convolutional neural networks (CNNs) on detection in the last decade. As the datasets and bench-marking sites are published, research direction has shifted towards yielding best accuracy on generic scenarios including re-identification
(reID) of objects while tracking. In this study, we narrow the scope of MOT for surveillance by providing a dedicated dataset of pedestrians and focus on in-depth analyses of well performing multi-object trackers to observe the weak and strong sides of state-of-the-art (SOTA) techniques for real-world applications. For this purpose, we introduce SOMPT22 dataset; a new set for multi person tracking with annotated short videos
captured from static cameras located on poles with 6-8 meters in height positioned for city surveillance. This provides a more focused and specific benchmarking of MOT for outdoor surveillance compared to public MOT datasets. We analyze MOT trackers classified as one-shot and two-stage with respect to the way of use of detection and reID networks on
this new dataset. The experimental results of our new dataset indicate that SOTA is still far from high efficiency, and single-shot trackers are good candidates to unify fast execution and accuracy with competitive performance.</p>

<h2 id="detection--tracking-datasets">Detection &amp; Tracking Datasets</h2>

<figure class="figure  figure--center">
  <img class="image" src="/assets/paper_pics/table1_1.png" alt="" width="1920" height="1000" />
  
</figure>

<figure class="figure  figure--center">
  <img class="image" src="/assets/paper_pics/table2_2.png" alt="" width="1920" height="1000" />
  
</figure>

<h2 id="sompt22-statistics">SOMPT22 Statistics</h2>

<figure class="figure  figure--center">
  <img class="image" src="/assets/paper_pics/fig1_3.png" alt="" width="1920" height="1000" />
  
</figure>

<figure class="figure  figure--center">
  <img class="image" src="/assets/paper_pics/table3_4.png" alt="" width="1920" height="1000" />
  
</figure>

<h2 id="experiment-setup">Experiment Setup</h2>

<figure class="figure  figure--center">
  <img class="image" src="/assets/paper_pics/table4-5.png" alt="" width="1920" height="1000" />
  
</figure>

<h2 id="benchmark-results">Benchmark Results</h2>

<figure class="figure  figure--center">
  <img class="image" src="/assets/paper_pics/table6-7.png" alt="" width="1920" height="1000" />
  
</figure>

<figure class="figure  figure--center">
  <img class="image" src="/assets/paper_pics/table8.png" alt="" width="1920" height="1000" />
  
</figure>

<figure class="figure  figure--center">
  <img class="image" src="/assets/paper_pics/table9.png" alt="" width="1920" height="1000" />
  
</figure>

<figure class="figure  figure--center">
  <img class="image" src="/assets/paper_pics/fig2.png" alt="" width="1920" height="1000" />
  
</figure>

<h2 id="citation">Citation</h2>

<div class="language-js highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">@</span><span class="nd">misc</span><span class="p">{</span><span class="na">https</span><span class="p">:</span><span class="c1">//doi.org/10.48550/arxiv.2208.02580,</span>
  <span class="nx">doi</span> <span class="o">=</span> <span class="p">{</span><span class="mf">10.48550</span><span class="o">/</span><span class="nx">ARXIV</span><span class="p">.</span><span class="mf">2208.02580</span><span class="p">},</span>
  <span class="nx">url</span> <span class="o">=</span> <span class="p">{</span><span class="na">https</span><span class="p">:</span><span class="c1">//arxiv.org/abs/2208.02580},</span>
  <span class="nx">author</span> <span class="o">=</span> <span class="p">{</span><span class="nx">Simsek</span><span class="p">,</span> <span class="nx">Fatih</span> <span class="nx">Emre</span> <span class="nx">and</span> <span class="nx">Cigla</span><span class="p">,</span> <span class="nx">Cevahir</span> <span class="nx">and</span> <span class="nx">Kayabol</span><span class="p">,</span> <span class="nx">Koray</span><span class="p">},</span>
  <span class="nx">keywords</span> <span class="o">=</span> <span class="p">{</span><span class="nx">Computer</span> <span class="nx">Vision</span> <span class="nx">and</span> <span class="nx">Pattern</span> <span class="nx">Recognition</span> <span class="p">(</span><span class="nx">cs</span><span class="p">.</span><span class="nx">CV</span><span class="p">),</span> <span class="na">FOS</span><span class="p">:</span> <span class="nx">Computer</span> <span class="nx">and</span> <span class="nx">information</span> <span class="nx">sciences</span><span class="p">,</span> <span class="na">FOS</span><span class="p">:</span> <span class="nx">Computer</span> <span class="nx">and</span> <span class="nx">information</span> <span class="nx">sciences</span><span class="p">},</span>
  <span class="nx">title</span> <span class="o">=</span> <span class="p">{</span><span class="na">SOMPT22</span><span class="p">:</span> <span class="nx">A</span> <span class="nx">Surveillance</span> <span class="nx">Oriented</span> <span class="nx">Multi</span><span class="o">-</span><span class="nx">Pedestrian</span> <span class="nx">Tracking</span> <span class="nx">Dataset</span><span class="p">},</span>
  <span class="nx">publisher</span> <span class="o">=</span> <span class="p">{</span><span class="nx">arXiv</span><span class="p">},</span>
  <span class="nx">year</span> <span class="o">=</span> <span class="p">{</span><span class="mi">2022</span><span class="p">},</span>
  <span class="nx">copyright</span> <span class="o">=</span> <span class="p">{</span><span class="nx">Creative</span> <span class="nx">Commons</span> <span class="nx">Attribution</span> <span class="nx">Non</span> <span class="nx">Commercial</span> <span class="nx">Share</span> <span class="nx">Alike</span> <span class="mf">4.0</span> <span class="nx">International</span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<h2 id="license">License</h2>

<p>The annotations of SOMPT22 are licensed under a <a href="https://creativecommons.org/licenses/by/4.0" title="Creative Commons Attribution 4.0 License">Creative Commons Attribution 4.0 License</a>. The dataset of SOMPT22 is available for non-commercial research purposes only. All videos and images of DanceTrack are obtained from the Internet.</p>
:ET